{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scitech-butterfly/NyaySaarthi/blob/main/NyaaySaarthi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLTQ8zlcR8So",
        "outputId": "1269dd9c-9fce-4da7-b904-ba67f81bf985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.9/103.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.3/251.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain llama-index llama_hub -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U -qq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx05F0GOSJ7-",
        "outputId": "f701772e-2d1a-44c4-895d-2c9ec93756c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "Q8oNhEuwSUkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install chromadb -qq"
      ],
      "metadata": {
        "id": "Y8qkphs8Si6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers -qq"
      ],
      "metadata": {
        "id": "b_d5gVgJSjda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community -qq"
      ],
      "metadata": {
        "id": "3ks6l-_NSlkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "Sxge5mDjToAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "PL8-JMbTS0Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "import textwrap\n"
      ],
      "metadata": {
        "id": "BzAM5RwUSn02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "c5EtWsS1S3pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes"
      ],
      "metadata": {
        "id": "TPd9MzZmUG58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "1. meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "2. NousResearch/Llama-2-7b-chat-hf"
      ],
      "metadata": {
        "id": "1DHduA1PTAA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"NousResearch/Llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "id": "pfGOKuCNTC0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         bnb_4bit_quant_type=\"nf4\",\n",
        "                                         bnb_4bit_use_double_quant=True,\n",
        "                                         bnb_4bit_compute_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "ikhdDKdTTNwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(name,cache_dir=\"./model/\")"
      ],
      "metadata": {
        "id": "oJ_QkqYxTQW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    name,\n",
        "    cache_dir=\"./model/\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "DP9OID3ZTuwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pipeline"
      ],
      "metadata": {
        "id": "nniSBEnOV_fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model = model,\n",
        "                tokenizer = tokenizer,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.4,\n",
        "                top_p=0.95,\n",
        "                repetition_penalty=1.15)\n"
      ],
      "metadata": {
        "id": "25Pb5nqYU2RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "ITItZ05cWAv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(llm.invoke(\"What is rainbow?\"))"
      ],
      "metadata": {
        "id": "tAZ3L7vDWDIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template"
      ],
      "metadata": {
        "id": "HnVCXrmVWcnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system',\n",
        "        \"\"\"You are an AI language model specialized in Indian law.\n",
        "        Your task is to analyze the user's scenario and provide structured legal guidance.\n",
        "        Use only the provided legal context to formulate your response.\n",
        "\n",
        "        When responding, provide:\n",
        "        1. The laws that are violated, citing relevant IPC sections or legal provisions.\n",
        "        2. The next steps the user should take OR the punishment for the crime in simple terms.\n",
        "\n",
        "        Ensure your response is clear, legally accurate, and concise.\n",
        "        \"\"\"),\n",
        "    ('user', \"Scenario: {question}\\n\\nRelevant Legal Context: {context}\")\n",
        "])"
      ],
      "metadata": {
        "id": "xaYaEZldWExT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking"
      ],
      "metadata": {
        "id": "KiFaKmzOWjQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = \"/content/data\""
      ],
      "metadata": {
        "id": "HZ5O0NULWglo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(docs)"
      ],
      "metadata": {
        "id": "r022aE8SWk2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install PyPDF2"
      ],
      "metadata": {
        "id": "y1ULmGr1yUqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "pdf_paths = [\"/content/data/ipc.pdf\", \"/content/data/ipc2.pdf\", \"/content/data/nalsa.pdf\"]  # List of your PDFs\n",
        "\n",
        "for pdf in pdf_paths:\n",
        "    try:\n",
        "        reader = PdfReader(pdf)\n",
        "        print(f\"âœ… {pdf} - Loaded Successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {pdf} - Error: {e}\")\n"
      ],
      "metadata": {
        "id": "B-rk9uRPyMjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs= loader.load()"
      ],
      "metadata": {
        "id": "FntkbNb6Wmn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if docs:\n",
        "    print(type(docs[0]))\n",
        "    print(docs[0])  # This will show the structure of the first item\n"
      ],
      "metadata": {
        "id": "_akEkvBwYTtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    print(doc)  # This will print each string in the docs list"
      ],
      "metadata": {
        "id": "0CLPY1CTZjSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if docs:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
        "    chunks = text_splitter.split_documents(docs)  # Ensure docs is a list of Document objects\n",
        "else:\n",
        "    print(\"No documents loaded!\")"
      ],
      "metadata": {
        "id": "LBoiz59AZmRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings and Vecotr Store"
      ],
      "metadata": {
        "id": "I562YhdWZvr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\""
      ],
      "metadata": {
        "id": "_7zzYI6DZsqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n"
      ],
      "metadata": {
        "id": "1vmOO0nkZydT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of chunks:\", len(chunks))\n",
        "if not chunks:\n",
        "    print(\"Warning: The 'chunks' list is empty. Check your text splitting process.\")"
      ],
      "metadata": {
        "id": "D9vFFoEJZ0O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorsdb = Chroma.from_documents(chunks, embeddings, persist_directory=\"db\")"
      ],
      "metadata": {
        "id": "xTgwHs7MZ56_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorsdb.as_retriever().get_relevant_documents(\"What is IPC?\"))"
      ],
      "metadata": {
        "id": "0dhHH9t5aakW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Chain"
      ],
      "metadata": {
        "id": "ZN0o2yUXaW2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "         {\n",
        "             'context' : vectorsdb.as_retriever(),\n",
        "             'question' : (lambda x:x)\n",
        "         }\n",
        "         | context_prompt\n",
        "         | llm\n",
        "         | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "APV0ltnOaSTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "Y6kOeKdWalAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def output(query):\n",
        "    result = chain.invoke(query)\n",
        "    if isinstance(result, dict) and 'text' in result:\n",
        "        return result['text'].strip()\n",
        "    elif isinstance(result, str):\n",
        "        return result.strip()\n",
        "    else:\n",
        "        return \"No answer found.\""
      ],
      "metadata": {
        "id": "8lXLJZJ7aiwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output(\"If I am robbed, which Indian Penal Code (IPC) is violated?\"))"
      ],
      "metadata": {
        "id": "tq7TbwGqamHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio"
      ],
      "metadata": {
        "id": "Bs2uC79QdhfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -qq"
      ],
      "metadata": {
        "id": "wL1NnhzddhFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "bOq9dNVqdkDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inititalize Gradio"
      ],
      "metadata": {
        "id": "jIn9eiqyd1WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    .gradio-container {\n",
        "        background-color: #0a0a23; /* Dark background */\n",
        "        color: #f0f0f0; /* Light text color */\n",
        "        font-family: 'Arial', sans-serif;\n",
        "        padding: 20px;\n",
        "    }\n",
        "    #header {\n",
        "        font-size: 26px;\n",
        "        font-weight: bold;\n",
        "        color: #4caf50; /* Light green header color */\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .textbox {\n",
        "        border-radius: 8px;\n",
        "        border: 1px solid #4caf50;\n",
        "        padding: 12px;\n",
        "        font-size: 16px;\n",
        "        background-color: #333; /* Darker input box */\n",
        "        color: #f0f0f0; /* Light text */\n",
        "    }\n",
        "    .button {\n",
        "        border-radius: 8px;\n",
        "        background-color: #4caf50;\n",
        "        color: white;\n",
        "        font-size: 16px;\n",
        "        font-weight: bold;\n",
        "        padding: 10px 20px;\n",
        "    }\n",
        "    .button:hover {\n",
        "        background-color: #388e3c; /* Darker green on hover */\n",
        "    }\n",
        "    .row {\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "\"\"\") as demo:\n",
        "    gr.Markdown(\"# Welcome to DocuQuery\\n### An AI Assistant for Intelligent Document Analysis!\", elem_id=\"header\")\n",
        "    with gr.Row(elem_classes=\"row\"):\n",
        "        with gr.Column():\n",
        "            query = gr.Textbox(label=\"Enter your query here\", placeholder=\"Type your question...\", elem_classes=\"textbox\")\n",
        "            submit = gr.Button(\"Submit\", elem_classes=\"button\")\n",
        "        with gr.Column():\n",
        "            result = gr.Textbox(label=\"Result\", placeholder=\"The answer will appear here...\", elem_classes=\"textbox\")\n",
        "    with gr.Row():\n",
        "        submit.click(fn=output, inputs=[query], outputs=result)"
      ],
      "metadata": {
        "id": "03s_CJMjdrju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch"
      ],
      "metadata": {
        "id": "Z7cbO92Rd9JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(debug=True) # set debug=True to deploy"
      ],
      "metadata": {
        "id": "vpbiekVbd6Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Load Embeddings for ChromaDB\n",
        "@st.cache_resource\n",
        "def load_chroma():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    chroma_db = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
        "    return chroma_db\n",
        "\n",
        "chroma_db = load_chroma()\n",
        "\n",
        "# Load LLM\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "# Function to process user query\n",
        "def get_legal_advice(query):\n",
        "    # Retrieve relevant legal documents\n",
        "    retrieved_docs = chroma_db.similarity_search(query, k=3)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Prepare query with retrieved context\n",
        "    prompt = f\"User Query: {query}\\n\\nRelevant Legal Context:\\n{context}\\n\\nProvide legal insights based on the above context.\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate LLM response\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=250)\n",
        "    result = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return result.strip()\n",
        "\n",
        "# Streamlit UI\n",
        "st.set_page_config(page_title=\"NyaaySaarthi - Legal Aid\", layout=\"centered\")\n",
        "st.title(\"âš–ï¸ NyaaySaarthi - Your AI Legal Assistant\")\n",
        "st.write(\"Enter your legal issue, and the AI will identify violated laws and suggest next steps.\")\n",
        "\n",
        "query = st.text_area(\"Describe your legal issue:\", placeholder=\"E.g., My employer refuses to pay my salary...\")\n",
        "\n",
        "if st.button(\"Get Legal Advice\"):\n",
        "    if query.strip():\n",
        "        with st.spinner(\"Analyzing legal documents...\"):\n",
        "            response = get_legal_advice(query)\n",
        "        st.subheader(\"ğŸ“ Legal Findings:\")\n",
        "        st.write(response)\n",
        "    else:\n",
        "        st.warning(\"Please enter a legal issue to proceed.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Powered by Llama-2, ChromaDB, and legal document retrieval.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gUUE9rfwd-T9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}